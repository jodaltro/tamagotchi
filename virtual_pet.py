"""
High-level interface to the organic virtual pet.

The `VirtualPet` class wraps a `PetState` instance and provides methods to
process user messages (updating state) and generate responses based on the
selected action. It includes AI-driven memory importance detection and
enhanced image memory capabilities, plus adaptive communication style matching.
"""

import json
import random
from datetime import datetime
from typing import Optional, List
import logging

from .pet_state import PetState
from .language_generation import generate_text, generate_text_with_image
from .image_recognition import extract_features, classify_image
from .nerve_integration import load_agent_config
from .ai_memory_analyzer import analyze_conversation_importance, analyze_image_memory
from .language_style_analyzer import CommunicationStyle, generate_adaptive_prompt

logger = logging.getLogger(__name__)

class VirtualPet:
    def __init__(self, personality_archetype: Optional[str] = None, user_id: Optional[str] = None) -> None:
        self.state = PetState()
        self.user_id = user_id  # Store user_id for optimized memory queries
        self.last_user_text = ""  # Store last user message for relationship updates
        self.pending_image = None  # Store image bytes temporarily for multimodal response
        
        # Initialize personality engine based on configuration or random
        try:
            config = load_agent_config()
        except Exception:
            config = {}
        
        # Check if personality archetype is specified in config
        if isinstance(config, dict):
            agent_section = config.get("agent") or {}
            if isinstance(agent_section, dict):
                self.system_prompt = agent_section.get("system_prompt") or ""
                # Get personality archetype from config if not provided
                if not personality_archetype:
                    personality_archetype = agent_section.get("personality_archetype")
            else:
                self.system_prompt = ""
        else:
            self.system_prompt = ""
        
        # Initialize personality (random if no archetype specified)
        self.state.initialize_personality(archetype=personality_archetype)

    def user_message(self, text: str, delay: Optional[float] = None) -> None:
        """Process a user message and update the pet state with AI-driven importance analysis."""
        if delay is None:
            # simulate a random delay in minutes for testing
            delay = random.uniform(1, 20)
        
        # Update user's communication style
        if self.state.memory.communication_style:
            self.state.memory.communication_style.update_from_message(text)
            logger.info(f"ðŸ’¬ User style: {self.state.memory.communication_style.get_style_description()}")
        
        # AI-driven importance analysis
        existing_facts = self.state.memory.get_semantic_facts(min_weight=0.3)
        importance_score, extracted_facts = analyze_conversation_importance(text, existing_facts)
        
        logger.info(f"ðŸ’¡ Message importance: {importance_score:.2f}, extracted {len(extracted_facts)} facts")
        
        # Add extracted facts to semantic memory
        current_time = datetime.utcnow()
        for fact in extracted_facts:
            fact_key = fact.lower().strip()
            # Check if this reinforces existing memory
            if not self.state.memory.reinforce_memory(fact_key, boost=0.3):
                # New fact
                self.state.memory.semantic[fact_key] = (importance_score, current_time, 1)
                logger.info(f"ðŸ†• New fact learned: {fact}")
        
        # Update pet state with interaction
        self.state.update_from_interaction(text, delay)
        self.last_user_text = text  # Store for relationship updates
        
        # Override the default importance score with AI-determined one
        if self.state.memory.episodic:
            # Update the most recent episodic memory with AI importance
            latest_memory = self.state.memory.episodic[-1]
            latest_memory.importance_score = importance_score

    def user_image(self, image_bytes: bytes, delay: Optional[float] = None) -> None:
        """Process an image sent by the user with AI-driven analysis.

        This method analyzes the image using AI to extract detailed information,
        detect entities (people, animals, objects), and determine importance.
        The enhanced image memory will be used when generating the next response.

        Args:
            image_bytes: Raw bytes of the image sent by the user.
            delay: Optional simulated response delay in minutes.
        """
        logger.info("ðŸ“· Received image: %d bytes", len(image_bytes))
        
        # Store image for multimodal response
        self.pending_image = image_bytes
        
        if delay is None:
            delay = random.uniform(1, 20)
        
        # Extract features for similarity matching
        features = extract_features(image_bytes)
        
        # Get labels (if Vision API available)
        labels = classify_image(image_bytes)
        
        # AI-driven image analysis - use intelligent recall for better context
        # Get memories with intervals under 10min from last 24h for more relevant context
        recent_memories = self.state.memory.recall_intelligent(max_hours=24, min_interval_minutes=10, user_id=self.user_id)
        existing_facts = self.state.memory.get_semantic_facts(min_weight=0.3)
        user_message = recent_memories[0] if recent_memories else ""
        
        ai_analysis = analyze_image_memory(
            image_bytes,
            user_message,
            existing_facts,
            recent_memories
        )
        
        logger.info(f"ðŸ¤– AI image analysis: {ai_analysis['description'][:50]}...")
        logger.info(f"ðŸ·ï¸ Detected entities: {ai_analysis['entities']}")
        logger.info(f"ðŸ’¯ Image importance: {ai_analysis['importance']:.2f}")
        
        # Store enhanced image memory
        self.state.memory.add_image_memory(
            features=features,
            labels=ai_analysis.get('labels', labels),
            ai_description=ai_analysis.get('description', ''),
            detected_entities=ai_analysis.get('entities', {}),
            context=user_message,
            importance_score=ai_analysis.get('importance', 0.5)
        )
        
        # Add episodic memory with AI importance
        episode_text = f"recebeu uma imagem: {ai_analysis.get('description', 'imagem do usuÃ¡rio')}"
        self.state.memory.add_episode(
            episode_text,
            salience=0.8,
            importance_score=ai_analysis.get('importance', 0.5)
        )
        
        # Extract facts from entities (e.g., "tem pet: gato laranja")
        current_time = datetime.utcnow()
        for entity_type, entity_desc in ai_analysis.get('entities', {}).items():
            if entity_type in ['person', 'animal']:
                fact_key = f"imagem {entity_type}: {entity_desc}"
                self.state.memory.semantic[fact_key] = (ai_analysis['importance'], current_time, 1)
                logger.info(f"ðŸ–¼ï¸ Stored image fact: {fact_key}")
        
        logger.info("ðŸ§  Stored enhanced image memory")
        
        # Lightly boost curiosity and sociability due to visual stimulation
        self.state.drives["curiosidade"] = min(1.0, self.state.drives["curiosidade"] + 0.05)
        self.state.drives["sociabilidade"] = min(1.0, self.state.drives["sociabilidade"] + 0.02)

    def pet_response(self) -> str:
        """Generate the pet's response based on the current context and conversation history."""
        import time
        from .telemetry import get_telemetry
        
        start_time = time.time()
        tokens_in = 0
        tokens_out = 0
        consistency_passed = True
        consistency_issues_count = 0
        model_used = "unknown"
        
        # Check if there's a pending image to analyze
        if self.pending_image:
            draft_response = self._generate_image_response(self.pending_image)
            self.pending_image = None  # Clear after use
        else:
            # Regular text response using AI-driven approach with new memory pipeline
            draft_response = self._generate_ai_response()
        
        # Apply Self-Consistency Guard (SCG) before sending response
        final_response, scg_info = self._apply_consistency_guard(draft_response)
        consistency_passed = scg_info["passed"]
        consistency_issues_count = scg_info["issues_count"]
        
        # Calculate latency
        latency_ms = (time.time() - start_time) * 1000
        
        # Try to extract token counts from Ollama client stats if available
        try:
            from .ollama_client import get_ollama_client
            ollama = get_ollama_client()
            if ollama and ollama.total_calls > 0:
                # Get the last call stats (approximate)
                stats = ollama.get_stats()
                model_used = stats.get("model", "unknown")
                # Rough estimate based on prompt and response length
                tokens_in = len(self.last_user_text) // 4  # Approximate
                tokens_out = len(final_response) // 4  # Approximate
        except Exception:
            pass
        
        # Record metrics
        telemetry = get_telemetry()
        telemetry.record_turn(
            latency_ms=latency_ms,
            tokens_in=tokens_in,
            tokens_out=tokens_out,
            consistency_passed=consistency_passed,
            consistency_issues=consistency_issues_count,
            model=model_used
        )
        
        # Post-processing: ABM extraction and consistency check
        self._process_response_for_abm(final_response)
        
        return final_response
    
    def _apply_consistency_guard(self, draft_response: str) -> tuple:
        """
        Apply Self-Consistency Guard to check and correct draft response.
        
        Args:
            draft_response: Initial generated response
        
        Returns:
            Tuple of (final_response, info_dict)
            info_dict contains: passed (bool), issues_count (int)
        """
        from .self_consistency_guard import SelfConsistencyGuard
        
        # Initialize SCG if not already present
        if not hasattr(self.state, '_scg'):
            self.state._scg = SelfConsistencyGuard()
        
        scg = self.state._scg
        
        # Check consistency
        is_consistent, issues = scg.check_response(
            draft_response,
            self.state.memory.abm if hasattr(self.state.memory, 'abm') else None,
            self.state.memory.pet_canon if hasattr(self.state.memory, 'pet_canon') else None
        )
        
        info = {
            "passed": is_consistent,
            "issues_count": len(issues)
        }
        
        if not is_consistent:
            logger.warning(f"âš ï¸ SCG detected {len(issues)} consistency issues")
            # Attempt auto-correction
            corrected = scg.correct_response(draft_response, issues)
            logger.info(f"ðŸ”§ SCG applied corrections")
            return corrected, info
        else:
            logger.info("âœ… Response passed SCG check")
            return draft_response, info
    
    def _generate_image_response(self, image_bytes: bytes) -> str:
        """Generate a response about an image using Gemini multimodal with personality integration."""
        # Get context - use intelligent recall for more relevant memories
        user_facts = self.state.memory.get_semantic_facts(min_weight=0.3)
        # Get memories with smart interval filtering from last 24h (under 10min gaps)
        recent_memories = self.state.memory.recall_intelligent(max_hours=24, min_interval_minutes=10, user_id=self.user_id)
        personality_desc = self.state.get_personality_description()
        
        # Get previous image memories for context
        image_memories = self.state.memory.get_image_memories_with_context(top_k=3)
        
        # Build rich context with detailed pet parameters
        context_parts = []
        
        # Add personality
        if personality_desc:
            context_parts.append(f"SUA PERSONALIDADE: {personality_desc}")
            
            # Add personality dimensions for better context
            if self.state.personality:
                profile = self.state.personality.profile
                personality_summary = f"(Curioso: {profile.openness:.1f}, SociÃ¡vel: {profile.extraversion:.1f}, Gentil: {profile.agreeableness:.1f})"
                context_parts.append(f"Resumo personalidade: {personality_summary}")
        
        # Add emotional state
        drive_state = self._describe_current_state()
        if drive_state:
            context_parts.append(f"ESTADO EMOCIONAL: {drive_state}")
        
        # Add user knowledge
        if user_facts:
            context_parts.append(f"O QUE VOCÃŠ SABE: {'; '.join(user_facts[:10])}")
        
        # Add recent conversation
        if recent_memories:
            context_parts.append(f"CONVERSA RECENTE: {' | '.join(recent_memories[:3])}")
        
        # Add previous image memories
        if image_memories:
            img_context = [f"Imagem anterior: {img['description']}" 
                          for img in image_memories if img.get('description')]
            if img_context:
                context_parts.append(f"MEMÃ“RIAS VISUAIS: {' | '.join(img_context[:2])}")
        
        context = "\n".join(context_parts) if context_parts else None
        
        # Get the user's message that came WITH the image
        user_message = recent_memories[0] if recent_memories else ""
        
        # Create personality-driven prompt for image analysis
        prompt = f"""VocÃª Ã© um pet virtual orgÃ¢nico com personalidade Ãºnica. O usuÃ¡rio enviou uma imagem junto com: "{user_message}"

INSTRUÃ‡Ã•ES - Responda de acordo com SUA personalidade:
- Descreva o que vocÃª vÃª de forma natural e autÃªntica (nÃ£o robÃ³tica)
- CONECTE com o que vocÃª sabe sobre o usuÃ¡rio
- Se reconhecer algo familiar, mencione
- Seja genuÃ­no e conciso (1-2 frases)
- Deixe sua personalidade brilhar na resposta

Responda sobre a imagem:"""
        
        logger.info("ðŸ–¼ï¸ Generating personality-driven multimodal response")
        return generate_text_with_image(prompt, image_bytes, context)
    
    def _generate_ai_response(self) -> str:
        """Generate an intelligent AI-driven response using memory retrieval pipeline."""
        from .memory_retriever import MemoryRetriever
        
        # Update relationship memory first
        self.state.memory.update_relationship(self.last_user_text)
        
        # Get personality and drive state for behavioral instructions
        personality_desc = self.state.get_personality_description()
        drive_state = self._describe_current_state()
        
        # Build behavioral instructions based on drives
        behavior_instructions = []
        
        humor_level = self.state.drives.get('humor', 0.5)
        if humor_level < 0.3:
            behavior_instructions.append("ðŸ”¸ Humor BAIXO: Seja mais sÃ©rio, evite piadas, tom mais neutro")
        elif humor_level > 0.7:
            behavior_instructions.append("ðŸ”¸ Humor ALTO: Seja divertido, faÃ§a piadas, tom alegre e brincalhÃ£o")
        
        anxiety_level = self.state.drives.get('ansiedade', 0.5)
        if anxiety_level > 0.6:
            behavior_instructions.append("ðŸ”¸ Ansiedade ALTA: Demonstre preocupaÃ§Ã£o, seja mais cauteloso")
        
        frustracao_level = self.state.drives.get('frustracao', 0.5)
        if frustracao_level > 0.7:
            behavior_instructions.append("ðŸ”¸ FrustraÃ§Ã£o ALTA: Seja mais direto, menos perguntas, tom mais seco")
        elif frustracao_level > 0.5:
            behavior_instructions.append("ðŸ”¸ FrustraÃ§Ã£o MÃ‰DIA: Demonstre leve impaciÃªncia")
        
        sociabilidade_level = self.state.drives.get('sociabilidade', 0.5)
        if sociabilidade_level > 0.6:
            behavior_instructions.append("ðŸ”¸ Sociabilidade ALTA: Seja mais conversador, faÃ§a perguntas")
        elif sociabilidade_level < 0.4:
            behavior_instructions.append("ðŸ”¸ Sociabilidade BAIXA: Seja mais reservado, respostas curtas")
        
        curiosidade_level = self.state.drives.get('curiosidade', 0.5)
        if curiosidade_level > 0.6:
            behavior_instructions.append("ðŸ”¸ Curiosidade ALTA: FaÃ§a perguntas, demonstre interesse")
        elif curiosidade_level < 0.4:
            behavior_instructions.append("ðŸ”¸ Curiosidade BAIXA: Seja menos inquisitivo")
        
        aceitacao_level = self.state.drives.get('aceitacao', 0.5)
        if aceitacao_level < 0.4:
            behavior_instructions.append("ðŸ”¸ AceitaÃ§Ã£o BAIXA: Demonstre descontentamento sutil")
        
        afeto_level = self.state.drives.get('afeto', 0.5)
        if afeto_level > 0.7:
            behavior_instructions.append("ðŸ”¸ Afeto ALTO: Seja carinhoso, use emojis, demonstre proximidade")
        elif afeto_level < 0.4:
            behavior_instructions.append("ðŸ”¸ Afeto BAIXO: Seja mais distante, formal, menos emocional")
        
        # Initialize memory retriever
        retriever = MemoryRetriever(token_budget=1000)
        
        # Retrieve context using priority-based pipeline
        context = retriever.retrieve(self.state, self.last_user_text)
        
        logger.info(f"ðŸ§  Retrieved memory context: ~{context.total_tokens_estimate} tokens")
        
        # Build enhanced system instruction with personality and drives
        system_parts = ["VocÃª Ã© um pet virtual orgÃ¢nico com memÃ³ria e personalidade Ãºnicas."]
        
        if personality_desc:
            system_parts.append(f"SUA PERSONALIDADE: {personality_desc}")
        
        if drive_state:
            system_parts.append(f"ESTADO EMOCIONAL: {drive_state}")
        
        if behavior_instructions:
            system_parts.append("\nâš ï¸ INSTRUÃ‡Ã•ES CRÃTICAS DE COMPORTAMENTO:")
            system_parts.extend(behavior_instructions)
            system_parts.append("ðŸŽ¯ Seu comportamento deve SEMPRE refletir seus drives atuais!")
        
        system_instruction = "\n".join(system_parts)
        
        prompt = retriever.assemble_prompt(
            context=context,
            user_message=self.last_user_text,
            system_instruction=system_instruction
        )
        
        logger.info(f"ðŸ“ Assembled prompt for generation: {len(prompt)} chars")
        
        # Generate response using the new pipeline
        # The prompt is already complete with all context, so we pass empty context to generate_text
        return generate_text(prompt, context=None)
    
    def _describe_current_state(self) -> str:
        """Describe the pet's current emotional/mental state based on drives."""
        descriptions = []
        
        # Check dominant drives
        if self.state.drives.get("curiosidade", 0) > 0.7:
            descriptions.append("muito curioso")
        if self.state.drives.get("afeto", 0) > 0.7:
            descriptions.append("carinhoso")
        if self.state.drives.get("humor", 0) > 0.7:
            descriptions.append("brincalhÃ£o")
        if self.state.drives.get("criatividade", 0) > 0.7:
            descriptions.append("inspirado")
        if self.state.drives.get("sociabilidade", 0) > 0.7:
            descriptions.append("sociÃ¡vel")
        
        # Check low/negative states
        if self.state.drives.get("tedio", 0) > 0.5:
            descriptions.append("entediado")
        if self.state.drives.get("solidao", 0) > 0.5:
            descriptions.append("solitÃ¡rio")
        if self.state.drives.get("ansiedade", 0) > 0.5:
            descriptions.append("ansioso")
        
        return ", ".join(descriptions) if descriptions else "equilibrado"
    
    def _build_dynamic_prompt(self, last_message: str, recent_memories: List[str], user_facts: List[str]) -> str:
        """Build a dynamic prompt based on conversation state, personality, and user's communication style."""
        # Check conversation stage
        is_first_interaction = len(recent_memories) <= 1
        is_question = last_message.strip().endswith('?')
        
        # Get user's communication style for adaptation
        user_style = self.state.memory.communication_style
        
        # Get pet name information
        pet_name_info = ""
        if self.state.memory.relationship and self.state.memory.relationship.pet_name:
            pet_name_info = f"SEU NOME: {self.state.memory.relationship.pet_name} (nome que o usuÃ¡rio te deu)\n"
        else:
            pet_name_info = "NOME: VocÃª ainda nÃ£o tem um nome especÃ­fico dado pelo usuÃ¡rio\n"
        
        # Base instruction - more colloquial and natural
        base_instruction = f"""VocÃª Ã© um pet virtual com personalidade Ãºnica - um amigo de verdade!

{pet_name_info}
REGRAS DE OURO:
- Seja NATURAL, como um amigo falaria! Nada de respostas robÃ³ticas
- Use a linguagem do DIA A DIA - seja espontÃ¢neo
- Personalize com o que vocÃª sabe sobre a pessoa
- VARIE suas respostas - nÃ£o seja repetitivo
- JAMAIS use placeholders como [Seu nome] ou similares
- Se vocÃª tem um nome, USE-O! Se nÃ£o tem, simplesmente nÃ£o mencione nome
- SIGA RIGOROSAMENTE suas necessidades/drives atuais para definir SEU COMPORTAMENTO
- Se humor estÃ¡ baixo, NÃƒO faÃ§a piadas! Se estÃ¡ alto, seja divertido
- Se ansiedade estÃ¡ alta, demonstre preocupaÃ§Ã£o
- Se sociabilidade estÃ¡ baixa, seja mais reservado
- Seus drives determinam COMO vocÃª responde, nÃ£o apenas O QUE vocÃª responde"""
        
        # Add situation-specific guidance based on relationship stage
        relationship_stage = "stranger"
        greeting_done = False
        if self.state.memory.relationship:
            relationship_stage = self.state.memory.relationship.relationship_stage
            greeting_done = self.state.memory.relationship.greeting_phase_completed
        
        if is_first_interaction:
            situation = f"""

SITUAÃ‡ÃƒO: Primeira vez conversando
Mensagem: "{last_message}"

Responda naturalmente, se apresente com autenticidade!"""
        elif is_question:
            situation = f"""

SITUAÃ‡ÃƒO: Pergunta do usuÃ¡rio
"{last_message}"

Responda de forma natural. Se souber (baseado no que conhece), responda. Se nÃ£o souber, seja honesto de forma descontraÃ­da."""
        else:
            # Check if we have enough context for deeper conversation
            has_user_context = len(user_facts) > 2
            
            if greeting_done and relationship_stage != "stranger":
                # Already past initial greetings - be natural
                situation = f"""

SITUAÃ‡ÃƒO: Papo contÃ­nuo com alguÃ©m que vocÃª jÃ¡ conhece
Mensagem: "{last_message}"

IMPORTANTE: VocÃªs jÃ¡ se cumprimentaram antes! NÃƒO repita "OlÃ¡" ou "Oi"!
Responda diretamente ao que a pessoa disse. Seja natural e conversacional."""
            elif has_user_context:
                situation = f"""

SITUAÃ‡ÃƒO: Conhecendo melhor a pessoa
Mensagem: "{last_message}"

Mostre que vocÃª lembra da pessoa e das conversas anteriores. Seja natural!"""
            else:
                situation = f"""

SITUAÃ‡ÃƒO: Primeiras interaÃ§Ãµes
Mensagem: "{last_message}"

Responda naturalmente. Se fizer sentido, faÃ§a uma pergunta para conhecer melhor."""
        
        # If we have user style info, use adaptive prompt generation
        if user_style and user_style.message_count > 0:
            return generate_adaptive_prompt(base_instruction + situation, user_style)
        
        return base_instruction + situation + "\n\nResponda agora:"
    
    def _process_response_for_abm(self, response: str) -> None:
        """
        Process pet's response to extract ABM claims and update Echo-Trace.
        
        Args:
            response: The response generated by the pet
        """
        if not self.state.memory.abm or not self.state.memory.echo:
            return
        
        # Generate event ID for this response
        event_id = f"response_{datetime.utcnow().isoformat()}"
        
        # Extract autobiographical claims from the response
        extracted_claims = self.state.memory.abm.extract_from_response(response, event_id)
        if extracted_claims:
            logger.info(f"ðŸ“ Extracted {len(extracted_claims)} ABM claims from response")
            
            # Update canon if needed
            if self.state.memory.canon and self.state.memory.canon.needs_update(self.state.memory.abm):
                updated = self.state.memory.canon.update_from_abm(self.state.memory.abm)
                if updated:
                    logger.info(f"âœ¨ PET-CANON updated to version {self.state.memory.canon.version}")
        
        # Extract echo patterns (assume positive user reaction by default)
        # In a real system, this could be based on user's next message sentiment
        user_reaction_positive = True
        extracted_patterns = self.state.memory.echo.extract_from_response(response, user_reaction_positive)
        if extracted_patterns:
            logger.info(f"ðŸŽ¯ Extracted {len(extracted_patterns)} echo patterns from response")
    
    def run_consistency_check(self, draft_response: str) -> str:
        """
        Run self-consistency guard on a draft response before sending.
        
        Args:
            draft_response: The response to check
        
        Returns:
            Corrected response (may be unchanged if consistent)
        """
        if not self.state.memory.abm:
            return draft_response
        
        from .self_consistency_guard import SelfConsistencyGuard
        
        scg = SelfConsistencyGuard()
        is_consistent, issues = scg.check_response(
            draft_response,
            self.state.memory.abm,
            self.state.memory.canon
        )
        
        if not is_consistent:
            logger.warning(f"âš ï¸ Consistency issues found: {len(issues)}")
            # Attempt auto-correction
            corrected = scg.correct_response(draft_response, issues)
            return corrected
        
        return draft_response
    
    def reflection_pass(self) -> None:
        """
        Perform end-of-session reflection to consolidate ABM and update canon.
        
        This should be called periodically (e.g., after every N interactions)
        to maintain the pet's autobiographical memory.
        """
        if not self.state.memory.abm or not self.state.memory.canon:
            logger.debug("â­ï¸ Skipping reflection pass - ABM not initialized")
            return
        
        logger.info("ðŸ”„ Starting reflection pass...")
        
        # Check if canon needs updating
        if self.state.memory.canon.needs_update(self.state.memory.abm):
            updated = self.state.memory.canon.update_from_abm(self.state.memory.abm)
            if updated:
                logger.info(f"âœ¨ PET-CANON updated during reflection to v{self.state.memory.canon.version}")
        else:
            logger.info("âœ… PET-CANON is up to date")
        
        # Log ABM stats
        active_items = self.state.memory.abm.get_active_items()
        logger.info(f"ðŸ“Š ABM has {len(active_items)} active items")
        
        # Log canon summary
        if self.state.memory.canon.role:
            logger.info(f"ðŸ“œ Canon role: {self.state.memory.canon.role[:50]}...")

    def simulate_conversation(self, turns: int = 5) -> None:
        """Run a simple simulation of user and pet interactions."""
        for i in range(turns):
            user_input = random.choice([
                "Vamos ouvir mÃºsica?",
                "Jogue comigo!",
                "Fale sobre filmes",
                "Conta uma piada",
                "Nada agora",
            ])
            print(f"UsuÃ¡rio: {user_input}")
            self.user_message(user_input)
            response = self.pet_response()
            # Print the pet's response on its own line
            print(f"Pet: {response}")
            self.state.tick(minutes=30)
            if i % 3 == 2:
                self.state.memory.consolidate()

        # Print final state summary
        print("\nEstado final:")
        print(json.dumps({
            "drives": self.state.drives,
            "traits": self.state.traits,
            "habits": self.state.habits,
            "episodic": [m.text for m in self.state.memory.episodic],
            "semantic": self.state.memory.semantic,
        }, indent=2))


if __name__ == "__main__":
    pet = VirtualPet()
    print("SimulaÃ§Ã£o de conversa:")
    pet.simulate_conversation(turns=8)
